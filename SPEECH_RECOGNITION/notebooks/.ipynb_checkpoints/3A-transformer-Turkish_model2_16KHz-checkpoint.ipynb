{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7902340c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#    Test WER on Common Voice 6.1  self-reported    8.830\n",
    "#    Test CER on Common Voice 6.1    self-reported    2.370\n",
    "#    Test WER on Robust Speech Event - Dev Data    self-reported    32.810\n",
    "#    Test CER on Robust Speech Event - Dev Data    self-reported    11.220\n",
    "#    Test WER on Robust Speech Event - Test Data    self-reported    34.860"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027fff9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e508120",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "import soundfile as sf\n",
    "import time\n",
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"cahya/wav2vec2-large-xlsr-turkish-artificial-cv\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"cahya/wav2vec2-large-xlsr-turkish-artificial-cv\").to('cuda')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "90f10627",
   "metadata": {},
   "source": [
    "# load pretrained model offline\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"MODELS_SAVED/wav2vec2-large-xlsr-turkish-artificial-cv\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"MODELS_SAVED/wav2vec2-large-xlsr-turkish-artificial-cv\")#.to('cuda')\n",
    "\n",
    "# load pretrained model online\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"cahya/wav2vec2-large-xlsr-turkish-artificial-cv\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"cahya/wav2vec2-large-xlsr-turkish-artificial-cv\").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794663cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2f7cae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = 'tr/clips_flac/'\n",
    "\n",
    "liste_dir = os.listdir(path)\n",
    "liste_dir = sorted(liste_dir)[:50]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f57a18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "def get_word_error_rate(r, h):\n",
    "\n",
    "    \"\"\"\n",
    "    Given two list of strings how many word error rate(insert, delete or substitution).\n",
    "    \"\"\"\n",
    "    d = numpy.zeros((len(r) + 1) * (len(h) + 1), dtype=numpy.uint16)\n",
    "    d = d.reshape((len(r) + 1, len(h) + 1))\n",
    "    for i in range(len(r) + 1):\n",
    "        for j in range(len(h) + 1):\n",
    "            if i == 0:\n",
    "                d[0][j] = j\n",
    "            elif j == 0:\n",
    "                d[i][0] = i\n",
    "\n",
    "    for i in range(1, len(r) + 1):\n",
    "        for j in range(1, len(h) + 1):\n",
    "            if r[i - 1] == h[j - 1]:\n",
    "                d[i][j] = d[i - 1][j - 1]\n",
    "            else:\n",
    "                substitution = d[i - 1][j - 1] + 1\n",
    "                insertion = d[i][j - 1] + 1\n",
    "                deletion = d[i - 1][j] + 1\n",
    "                d[i][j] = min(substitution, insertion, deletion)\n",
    "    result = float(d[len(r)][len(h)]) / len(r) * 100\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aee3ec8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('tr/test.tsv',sep='\\t')\n",
    "liste_dir = test['path'].values\n",
    "good_text = test['sentence'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "823417df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('START')\n",
    "mean = []\n",
    "for i,liste  in tqdm(enumerate(liste_dir)):\n",
    "    data_batch = liste_dir[i]    \n",
    "    audio_input, sample_rate = sf.read(path+data_batch[:-4]+'.flac')\n",
    "    input_values = processor(audio_input, sampling_rate=16000, return_tensors=\"pt\").input_values.to('cuda')\n",
    "\n",
    "    # INFERENCE\n",
    "\n",
    "    ## retrieve logits & take argmax\n",
    "    logits = model(input_values).logits\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    ## transcribe\n",
    "    for predicted in predicted_ids:\n",
    "        transcription = processor.decode(predicted)\n",
    "        #print(transcription)\n",
    "        mean.append(get_word_error_rate(good_text[i], transcription))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
